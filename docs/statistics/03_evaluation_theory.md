---
next:
  text: 'К содержанию'
  link: '/statistics/statistics_index'
prev: false
outline: deep
---

# Теория  оценивания

**Определение.** Пусть . Любое измеримое отображение от реализации выборки назовем статистикой (оценкой)
>[!info] Измеримое множество: 
$$ \forall A \in B(\mathbb{R}^m): T(A) \in B(\mathbb{R}^n) $$

**Определение**. Оценка $T(x)$ называется *несмещенной* для параметра $\Theta$, если
$$E T(X) = \Theta$$
Для функции $\tau(\theta)$:
$$E T(x) = \tau(\theta)$$
**Определение**. Оценка $T(x)$ называется *состоятельной*, если она с ростом выборки сходится по вероятности к ецениваемому параметру
$$ T(x) \xrightarrow[n \to \infty ]{P} \Theta $$

## Выборочные моменты

**Определение**. Среднее значение (выборочное среднее) оценка для мат ожидания
$$ \alpha = \frac{1}{n} \sum_{i = 1}^n X_i $$

**Определение**. k-ым выборочным моментом по выборке $X_1 ... X_n$ назовем величину
$$ \alpha_k =\frac{1}{n} \sum_{i = 1}^n X_i^k $$

**Определение**. k-ым центральным моментом назовем 
$$ \alpha_k = \frac{1}{n} \sum_{i = 1}^n (X_i- \bar{X})^k $$
**Определение**.При $k = 2$ $\alpha_2$ называется *выборочной дисперсией*

**Утверждение 2.1**. Выборочные моменты $\alpha_k$ являются несмещенными и состоятельными оценками теоретических моментов $E \xi^k$.
**Доказательство**.

> $X_1 ... X_n$ - независимые случайные величины

1. Несмещенность оценки:
$$ E\left( \frac{1}{n} \sum X_{i}^k\right) = \frac{1}{n} \sum E X_{i}^k = \frac{1}{n} \cdot n E X_{1}^k = E X_{1}^k$$
Следовательно оценка несмещенная.

2. Состоятельность
$$ P\left( |\frac{1}{n} \sum X_{i}^k - E X_{1}^k| > \varepsilon \right) \to 0$$

Вспомним ЗБЧ в форме Хинчина для конечных последовательностей

>[!info] ЗБЧ в форме Хинчина для конечных последовательностей
> Для любой последовательности $\xi_{1} \dots \xi_{n}$независимых (в совокупности) и одинаково распределённых случайных величин с конечным первым моментом имеет место сходимость:
> $$ \frac{\xi_{1} + \dots + \xi_{n}}{n} \xrightarrow[]{P} E\xi_{1} $$

**Задача**. Проверить, что $D X \to 0$, для $F_\xi (x) \colon D \xi < \infty$.

Рассмотрим центральные моменты:
$$ \mu_{k}=\frac{1}{n} \sum_{i=1}^n (X_{i}-\bar{X})^k = \frac{1}{n} \sum_{i=1}^n \sum_{i=j}^k C^j_k X_i^j(-1)^{k-j}(\bar{X})^{k-j}$$
Решим задачу на частном примере:
$\mu_2$ - выборочная диспресия
$$ \mu_2 = \frac{1}{n} \sum_{i=1}^n (X_i - \alpha_1)^2 = \frac{1}{n} (\sum_{i=1}^n X_i^2 - 2 \sum_{i=1}^n X_i \alpha_1 + n \alpha_1^2) = \alpha_2 - 2 \alpha_1^2 + \alpha_1^2 = \alpha_2 - \alpha_1^2$$
Следовательно $\mu_2 \xrightarrow[]{P} D X_1$ при $n \to \infty$

$$ |\alpha_2 - (\alpha_1)^2 - (E X_1^2 - (E X_1)^2)| = \
| (\alpha_2 - E X_1^2) + ((\alpha_1)^2  - (E X_1)^2)| \leq \
|\alpha_2 - E X_1^2| + |(\alpha_1)^2  - (E X_1)^2|$$

Существует $n$ такое, что:

$$ P\left( |\alpha_{1} - E X_{1}^2| < \frac{\varepsilon}{2} \text{и} |\alpha_{2} - (E X_{1})^2| < \frac{\varepsilon}{2} \right) \to 1$$

Тогда и для меньшего подмножества утверждение будет верно:
$$ P(|\mu_2 - D X_1| < \varepsilon) \to 1$$

**Утверждение**: Будет ли эта оценка несмещенной? (спойлер: нет)
**Доказательство**:
Нужно показать, что математическое ожидание
$$ E \mu_2 = \frac{1}{n} \sum_{i=1}^n (X_i - \alpha_1) = \frac{n-1}{n} D X_1 $$

*Как сравнивать оценки между собой? → Как вариант, можно использовать среднеквадратичное отклонение*

Для статистики (оценки) $T(X)$, которая оценивает некоторый параметр $\tau(\theta)$. мерой её эффективности назовем ее мат. ожидание разности
$$E(T(X) - \tau(\theta))^2$$
Если статистика $\tau(x)$ - несмещенная для $\tau(\theta)$, то
$$E(T(X) - \tau(\theta))^2 = D \tau(\theta)$$

**Определение**. $T(X)$ назвается оптимальной оценкой, если среди всех оценок она обладает минимальной дисперсией.

**Утверждение**. Если оптимальная оценка существует, то она единственная
**Доказательство.** Предположим, что существует 2 оптимальные оценки: $T_1(x), T_2(x)$

Вследствие того, что эти оценки оптимальные, их дисперсии будут равны:
$$D (T_1(x)) = D (T_2(x))$$
Построим третью оценку:
$$T_3(x) = a_1 T_1(x) + a_2 T_2(x)$$
$$
\begin{array}{}
D T_3 = E (a_1 T_1(x) + a_2 T_2(x) - (a_1 E T_1(x) + a_2 E T_2(x)))^2 = \\
= E (a_1 (T_1(x) - E T_1(x)) + a_2 (T_2(x) - E T_2(x)))^2 = \\
= a_1 ^2 D \eta_1 + a_2 ^2 D \eta_2 + 2  \text{cov}(a_1 \eta_1, a_2 \eta_2) = \\
= a_1^2 D \eta_1 + a_2^2 D \eta_2 + 2 a_1 a_2 \text{cov}( \eta_1, \eta_2) = *
\end{array}
$$
Из неравенства Коши-Буняковското мы знаем, что $\text{cov} (\eta, \xi) \leq \sqrt{D \xi D \eta}$

Получим:
$$ * = a_1 ^2 D \eta_1 + a_2 ^2 D \eta_2 + 2 a_1 a_2 \sqrt{D \eta_1 D \eta_2} $$
Возьмем $a_1 \in (0;1), a_2 = 1 - a_1$. Если мы возьмем $T_3$  с такими параметрами, мы получим "более" оптимальную" оценку, чего не может быть. Следовательно оптимальная оценка едиственная.
