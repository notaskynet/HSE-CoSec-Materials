---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Градиентный спуск

$$
\nabla Q(w) = \begin{pmatrix}
 \frac{\partial{Q}}{ \partial{w_{1}}} \\
\vdots \\
\frac{\partial{Q}}{ \partial{w_{d}}}
\end{pmatrix} - \text{градиент}
$$
>[!info] Градиент показывает направление наибольшего возрастания функции

1. $w^{(0)}$ - инициализируем начальный вес
2. Шаг: $$ w^{(k)} = w^{(k-1)} - \eta \nabla Q (w^{(k-1)} )$$
3. Остановка
	1) $|| w^{(k)} - w^{(k-1)} || <\varepsilon$
	2) $|| Q(w^{(k)}) - Q(w^{(k-1)}) || <\varepsilon$
	3) $k > N$
	4) Если ошибка на валидационной выборке начала рости

### Проблемы градиентного спуска
1) Локальные минимумы → Как это можно решить? (Спойлер: в общем случае никак, но можно попытаться)
	- Мультистарт (Плохо работает на нейронных сетях)
	- Правильная инициализация весов
	- Использовать условия сходимости градиентного спуска (их много). Ниже пример:
>[!info] Условия сходимости градиентного спуска;
>- $Q(w)$ - выпуклая и дифференцируемая;
>- $Q(w)$ - липшецева: $$ | \nabla Q(w_{1}) - \nabla Q(w_{2}) | \leq L \cdot || w_{2} - w_{1}||$$
>- $\eta$ не очень большая;
>Если эти условия выполняются, то функция сходится к минимуму
###  Оценивание градиента

$$ Q(w) = \frac{1}{\ell} \sum_{i=1}^\ell L(y_{i}, a(x_{i})) $$
$$ \nabla Q(w) = \frac{1}{\ell} \sum_{i=1}^\ell \nabla q_{i} (w)$$
Проблема: Нужно вычислить $\ell$ градиентов $\implies$ Градиентный спуск медленный.

### Вариации градинетного спуска
#### Стохастический градиентный спуск
**Идея**: Выбираем случайно в каждой итерации один объект из выборки и производим по нему градиентный спуск
**Алгоритм**:
- $\nabla Q(w)=\nabla q_{i(w)}$
- Шаг SGD:
$$ i_{k} - \text{random object from sample}$$
$$ w^{(k)} = w^{(k-1)} - \eta \nabla q_{{i_{k}}} (w^{(k-1)})$$
- Условия сходимости:
>[!note] Условия Роббинса-Монро
>- $\sum_{k=1}^\infty \eta_{k}=\infty$
>- $\sum_{k=1}^\infty \eta_{k}^2<\infty$
>При выполнении условий SGD сойдется к минимуму
>**Примеры:**
>- $\eta = \frac{1}{k}$
>- $\eta=\lambda ( \frac{s_{0}}{s_{0}+k})^p, (s_{0}, \lambda, p)$
- Скорость сходимости:
$$ E (Q( w^{(k)}) - Q(w_{*})) = \bar{o}\left( \frac{1}{\sqrt{ k }} \right)$$
**Примечание**:
- Хуже сходимость, чем у полного градиентного спуска
- Может использовать одни и те же объекты по несколько раз
- SGD хорошо показывает себя в задачах онлайн-обучения (Мы считываем объекты с диска по одному и обучаемся на них)
#### Mini-Batch Gradient Decciend
**Идея**: Примерно такая же, как и у SGD, но используется несколько объектов (сходимость лучше, чем SGD, но хуже GD). также, чтобы каждый объект имел одинаковый вес при обучении, бвтчи выбираются последователльно, чтобы вся выборка поучаствовала в обучении. Цикл из итерациий по всей выборке называется *эпохой*.
