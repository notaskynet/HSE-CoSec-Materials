---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Линейная классифкация

$\mathbb{Y} = \{-1, +1\}$ - бинарная классификация
- $-1$ - отрицательрые объекты
- $+1$ - положительнын объекты

$$ a(x) = \text{sign} \langle w, x \rangle  $$
Что делать, если $\langle w, x \rangle = 0$:
- Такого не бывает(ошибка)
- Отказ от классификации
геометрия: $<w,x> = 0$ - гиперплоскость, где $w$ - вектор нормали

Линейный классификатор разделяет классы гиперплоскостью

Скалярное произведение по модулю тем больше, чем больше расстояние от гиперплоскости, т.е. чем больше расстояние, тем более модель уверена в своем ответе.
$Q(a) = \frac{1}{\ell} \sum [a(x_{i} \neq y_{i})] \to min_{w}$ - оптимизием индикатор
Проблемы:
- Она не $NP$-полная (номально не решается за полиномиальное время)
- Дважды не дифференцируемая

(Примечание $Q$ - ЭФР)

$$ \frac{1}{\ell} \sum_{i=1}^\ell [\text{sign} \langle w, x \rangle\neq y_{i}] = \frac{1}{\ell} \sum_{i=1}^\ell [y_{i}<w, x_{i} < 0] $$
Проверим:
$$
\begin{equation}
    \left\{  
    \begin{array}{}
 y_{i} \langle w, x_{i} \rangle > 0 \implies  y_{i} =\text{sign} \langle w, x \rangle \\
 y_{i} \langle w, x_{i} \rangle < 0 \implies  y_{i} \neq \text{sign} \langle w, x \rangle
    \end{array}  
    \right. 
\end{equation}
$$

$M = y_{i} \langle w, x_{i} \rangle$ - отступ (margin)
- Знак отступа показывает корректность классификации
- Величина отступа показывает уверенность модели в выборе

![](https://i.imgur.com/TNrcw3o.png)


Мы хотим придумать некоторое $\bar{L}$, которая оценивала сверху $L$, и при этом при минимизации $\bar{L}$ уменьшалась и $L$
$$ 0\leq \frac{1}{\ell} \sum_{i=1}^\ell [y_{i}<w, x_{i} < 0] \leq \sum_{i=1}^\ell \bar{L}(y_{i}\langle w, x_{i} \rangle) $$
Идеи для $\bar{L}$:
1. $\bar{L_{1}} (M) = \max(0, 1-M)$ - hinge-loss
2. $\bar{L_{2}} (M) = e^{-M}$
3. $\bar{L_{3}} (M) = \log (1 + e^{-M})$ - логистическая функция потерь
4. …

**Примечание**. Выбор конкреьной модели дает ей определенные свойства
