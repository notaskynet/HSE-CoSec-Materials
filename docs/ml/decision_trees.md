---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Решающие деревья

По английcки: Decision Trees

Линейные модели:
- Дифференуируемые => Можно обучать градиентные методы
- Линейные, что может быть минусом => Deep Learning

Но не все задачи дифференцируемые! (Пример: задача сортировки массива)

Будем придумывать недифференцируемые модели

![ ](https://i.imgur.com/8wPINHc.png)
> Пример решающего дерева

## Структура модели

Решающее дерево - это бинарное дерево, где:
1. $v$ - внутренняя вершина $\implies$ $\beta_{v} \colon \mathbb{X} \to \{0, 1\}$ - предикат
2. $v$ - листовая $\implies$ $c_{v} \in \mathbb{Y}$ - прогноз
Бывают multi-way деревья(на одну вершину приходится больше 2 предсказаний), но в них обучение сложнее (они легче переобучаются) и само по себе оно более трудозатратно.

Какие бываю предикаты?
- $\beta_{v}(x) = [x_{j} \geq t]$ - самый простой и частоиспользуемый вариант
- $\beta_{v}(x)=[\langle w, x \rangle \geq t]$ - обучаем линейную модель и смотрим значения, которые она принимает
- $\beta_{v}(x) = [\rho(x, x_{0}) \geq t]$ - расстояние до некоторой точки

Рассмотрим первый предикат:
![](https://i.imgur.com/SYMS73v.png)
> Каждый лист соответствует одной из областей

Какие бывают прогнозы?
- Регрессия $\implies c_{v} \in \mathbb{R}$
- Классификация $\implies c_{v} \in \{1\dots l\}$
- ...

## Как обучать деревья

**Утверждение**: Если в $\mathbb{X}$ нет $x_{i}=x_{j}$, $y_{i}\neq y_{j}$, то существует дерево с нулевой ошибкой на *обучающей* выборке.

**Гипотеза**: Если из всех корректных на обучающей выборке деревьев взять самое маленькое (по глубине, по числу вершин), то оно не будет переобучено. $\implies$ Это $NP$ трудная задача, поэтому решить оптимально не получится $\implies$ Нужны какие-то ухищрения $\implies$ Будем использовать жадные алгоритмы

```Псевдокод
SplitNode(m, Rm):
    """
    Аргументы
    - m - номер вершины
    - Rm - объекты из обучающей выборки, которые
      попадают в текущую вершину
    """
    Если выполнены критерий останова(?):
        с_m = ... # (Средний ответ Rm) или
                  # (Самый частый класс)
                  # (Доли классов в Rm) или ...
        Выход
    Иначе:
        j, t = Argmax(j in (1...d), t in R) Q(Rm, j, t)
        # Q - Функционал качества предикат
        #     или критерий информативности предиката
        # Примечание: Поиск j и t решается полным перебором
        Rl = {(x, y) in Rm | [x_j < t] = 1}
        Rr = {(x, y) in Rm | [x_j >= t] = 1}
        SplitNode(l, Rl):
        SplitNode(r, Rr):
```

*Критерий останова* может быть:
- Ограничение на максимальную глубину
- Максимизировать число объектов в листе
- Некоторая доля ошибок в листе
- ...

## Критерии качества предиката

$H(R)$ - impurity (хаотичность) - эта функция показывает разнородность ответов в R (смотреть семинары)

Примеры:
- Регрессия
$$H(R_{m}) = \frac{1}{|R_{m}|} \sum_{x, y \in R_{m}} (y_{i} 0 \bar{y}_{m})$$
- Классификация:
- Доля объектов $k$-того класса в вершине
$$p_{k} = \frac{1}{|R_{m}|} \sum_{x, y \in R_{m}} [y_{i} = y_{k}]$$
    Далее можно использ-овать разные метрики, например энтропию:
$$H(R) = \sum_{i=1}^{k} p_{k} \log p_{k}$$
    Или другие критерии, такие как, критерий Джини:
$$H(R) = \sum_{k=1}^{d} p_{k}(1 - p_{k}) $$

Есть общий подход к построению $H(R)$:

$$H(R) = \mathop{\text{min}}_{ c \in \mathbb{R} } \frac{1}{|R_{m}|} \sum_{x, y \in R_{m}} L(y_{i}, c)$$
Идея: Мы выбираем среди всех константу такую, что у нее минимальная ошибка из всех. Если констатой можно добиться низкой ошибки, то impurity будет низкая.

Как при этом считать функционал ошибки:
Идея: Считаем impurity до и после разбиения
$$Q(R_{m}, j, t) = H(R_{m}) - \frac{|R_{\ell}|}{|R_{m}|} H(R_{\ell}) - \frac{|R_{r}|}{|R_{m}|} H(R_{r})$$
Примечание: Нужно делать нормировку, чтобы не допускать разбиение по одному признаку.

Для оптимизации упростим выражение:
$$ \frac{|R_{\ell}|}{|R_{m}|} H(R_{\ell}) + \frac{|R_{r}|}{|R_{m}|} H(R_{r}) \to \mathop{\text{min}}_{ j, t }  $$
## "Стрижка деревьев" (pruning)

Идея: Когда мы обучаем одно дерево, то зачаствую мы получаем либо  недообученую, либо переобученную модель. Идея метода заключается в том, что мы берем переобученную модель и потом "отрезаем" у него некоторые ветки, чтобы итоговое качество не сильно падало. Один из популярных методов: cost-complexity pruning

## Обрабботка пропусков

**Вариант 1.**

Обучение: в $R$ у признака. $j$ есть пропуски.

$[x_j \geq t ]$ - как посчитать качество предиката

В чем проблема просто выкинуть эти признаки?

Проблема: чем меньше будет обектов тем проще будет разить подвыборку, чтобы была маленькая Q. Если выкинуть часть объектов, получится, что модель будет отдать приоритет признакам, где ольше пропусков

Можно использовать это разбиение, но при этом штрафовать модель за пропуски
$$Q(R, j, t) = \frac{|R\backslash V_{j}|}{|R|} Q(R\backslash V_{j}(R), j, t)$$

Если выбран предикат $j$-ым признаком, то отправляем объекты из $V_j(R)$ и влево, и вправо с весами $\frac{|R_{l}|}{|R|}$ и $\frac{|R_{r}|}{|R|}$.

Применение:

Пусть $a_{mk}(x)$ - прогноз вероятности класса $k$ в вершине $m$. Тогда:
$$a_{mk}(x) =
\begin{equation}
    \left[
    \begin{array}{lcl}  
        a_{lk}(x), \text{ if }\beta_{m}(x)=0\\  
        a_{rk}(x), \text{ if }\beta_{m}(x)=1\\  
         \frac{|R_{l}|}{|R_{m}|}a_{lk}(x) + \frac{|R_{r}|}{|R_{m}|}a_{rk}(x), \text{ if }\beta_{m}(x) \text{ if feature is missed } \\
  c_{mk} \text{ - target}, \text{ if m - leaf predicat}
    \end{array}  
    \right.
\end{equation}
$$

Вариант 2. Суррогатный предикат

1. Выбираем лучший предикат без учета пропусков: $\beta(x)$
2. Ищем несколько суррогатных предикатов (суррогатные предикаты - это предикаты, которые дают похожие результаты): $\beta'(x)$, $\beta''(x)$, ...
3. На этапе предсказания выбираем лучший предикат, который может предсказать без пропусков:
$$x \to \beta(x) \mathop{\to}_{\text{не получ.}} \beta'(x) \mathop{\to}_{\text{не получ.}} \beta''(x)\mathop{\to}_{\text{не получ.}}\dots $$

### Работа с категориальными признаками

$x_{j}$ - категориальный признак
$x_{j} \in Q=\{u_{1} \dots u_{q}\}$

Тривиальный вариант
$[x_{j} = q]$ - просто используем предикат равентства

Multi-way splits (больше 2 вариантов для каждого предиката)

Вариант по лучше:$Q = Q_{1} \sqcup Q_{2}$ - разбиваем на 2 не пересекающихся множества

$\beta(x) = [x_{j} \in Q_{1}]$ - предикат, который используем далее
Всего $2^{|Q|}$ разбиений, но можно найти оптимальное разбиение. Посмотрим как:

$R_{m}(u) = \{(x, y) \in R_{m} | x_{y} = u\}$
$N_{m}(u) = |R_{m}(u)|$
Сотрируем по категориям
Первая категорий меньше всего положительных объектов
$$\frac{1}{N_{m}(u_{1})} \sum_{(x_{i}, y_{i}) \in R_{m}(u_{1})} [y_{i} = +1] \leq \dots \leq \frac{1}{N_{m}(u_{q})} \sum_{(x_{i}, y_{i}) \in R_{m}(u_{q})} [y_{i} = +1]$$

Если перебрать подмножества из упорядоченные категорий, то найдется оптимальное разбиение (сложность с экспоненциальном перешла в линейную), но это раотает только для линейного случая 

Тоже самое можно считать для регрессии, тогда вместо индикатора класса там будет квадрат отклонения от среднего таргета

## Связь деревьев и линейных моделей

$$\mathbb{X} = J_{1} \sqcup \dots \sqcup J_{n}$$
$$w_{1} \dots w_{n} - \text{target in these areas}$$
$$\alpha(x) = \sum_{j=1}^{n} w_{j}[x_{j}=x]$$

Заметим, что индикатор тут выступает тут в поли как бы признак, а $w_{j}$ - вес. То есть дерево своими разбиениями находит новые, нелинейные отностительно исходных признаки, а потом на них строит линейную модель

**Идея**: Можно построить дерево со случайными предикатами, и дальше использовать номера листьев, куда попадают объекты как новые признаки (PmXTree)
