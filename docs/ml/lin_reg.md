---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Линейная регрессия

$$ a(x) = \sum_{i=1}^d w_i x_i + w_0 = \langle w, x \rangle$$
*Обозначения:*

- $w_i$ - веса модели
- $x_i$ - признаки
- $w_0$ -  bias intercept

**Применимость линейных моделей.**

- Признаки влияют независимо
- Признаки влияют линейно

## Признаки в линейных моделей

### Категориальные признаки

Пример: районы Москвы (Хамовники, Тверской, …)

$x_j \in \{C_1 \dots C_m\}$ - категориальный

Для того, чтобы работать с такими признаки можно использоватть *one-hot encoding*

- $b_1(x) \dots b_m(x)$
- $b_i(x) = [x_i = C_i]$
Есть еще подходы к кодированию категориальных признаков([[Mean Target Encoding]]). При этом, если признаков слишком много, можно группировать малочисленные группы.

**Бинаризация признаков.**

![alt](https://i.imgur.com/YB8UAh6.png)

В качестве признаков мы рассматриваем попадание в интервал:
$$ [0;t_0] [t_0;t_1] [t_1; t_2] ... [t_m; +infinity] $$
**Полиномиальные признаки**
Можно добавлять функции от переменных
$$ x \to f(x) $$
*Проблема*: не понятно, что с этим делать

### Обучение линейной регресии

 $$ Q(w) = \frac{1}{\ell} \sum_{i=1}^\ell (\langle w, x_{i}  \rangle-y_{i}) \to \min_{w}$$
 $$ \frac{1}{\ell} || Xw-y||^2 \to \min $$
 где:
  $$ X = \begin{pmatrix}   x_{11} & \dots & x_{1d} \\ \vdots{} \\  x_{\ell1} & \dots & x_{\ell d}  \end{pmatrix}, w = \begin{pmatrix} w_{1} \\ \vdots \\ w_{d} \end{pmatrix}, y = \begin{pmatrix} y_{1} \\ \vdots \\ y_{\ell} \end{pmatrix}$$
  Получим:
  $$ \nabla Q(w) = 0$$
  $$ w_{*}=(X^T X)^{-1}X^T y$$

1) Обращение матрицы имеет сложность $O(x^3)$
2) Если более сложная функция потерь, то решить уравнение не получится.

Поэтому для обучения линейной регресии чаще всего используются [градиентным спуском](gradient_desciend.md).
