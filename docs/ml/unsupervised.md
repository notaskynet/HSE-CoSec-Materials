---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Обучение без учителя (Unsupervised learning)

![alt](https://i.imgur.com/R9eNInw.png)

## Кластеризация

$\mathbb{X} = (x_{i})_{i=1}^{\ell}$
Хотим построить модель $a$, такую что:
$$a: \mathbb{X} \to \{1\dots K\}$$
При этом, если $x_{i} ~ x_{j} \implies a(x_{i})=a(x_{j})$

Зачем?
1. Развелочный анализ данных
2. Генерация признаков
    ![alt](https://i.imgur.com/An4NCgQ.png)
3. Квантизация данных (уменьшение размера ппространства конкретного признака)
4. Продуктовые задачи (например, кластеризация новостей)
### Метрики качества кластеризации
> Лучший способ: глазами

#### 1. Внутрекластерное расстояие
> $c_{k}$ - центр $k$-того класстера
$$\sum_{k=1}^{K} \sum_{i=1}^{\ell} [a(x_{i}=k)]\rho(x_{i}, C_{k}) \to \mathop{\text{min}}$$
![alt](https://i.imgur.com/BTl3OYL.png)

#### 2. Межкласстерное расстояние

$$\sum_{i\neq j}[a(x_{i})\neq a(x_{j})]pho(x_{i}, x_{j}) \to \mathop{\text{max}} $$
#### 3. Индекс Данна:
- $\alpha(k, k')$ - расстояние между класстерами
- $\alpha(k)$ - внутрекласстерное расстояние
$$\frac{\mathop{\text{min}}_{ 1\leq k < k' < K } \alpha(k, k') }{\mathop{\text{max}}_{ 1\leq k\leq K }\alpha(k) } \to \mathop{\text{max}}  $$
(Ну и многое другое...)

### Выбор числа класстеров
- есть методы, которые выбирают количество класстеров сами(но не все)
- есть методы, где число параметров - это гиперпараметр
#### 1. Метод локтя (elbow method)
![alt](https://i.imgur.com/PSnkT0r.png)

#### 2. Метрические методы
Для метрических методов необходимо задавать расстояние на объектах.
##### K Means
> K - гиперпараметр

$$\sum_{k=1}^{K} \sum_{i=1}^{\ell} [a(x_{i}) = k] \rho(x_{i}, c_{k}) \to \mathop{\text{min}} $$
**Проблема**: Мы не знаем ни центры классов, не конкретные предсказания на объектах

**Идея**: Будем поочередно оптимизировать по $a(x)$ и $c_{k}$

- Шаг 0: Инициализируем центры (например, k-means++)
- Далее:
  - Шаг а: Фиксируем $c_k$
$$a(x_{i}) = \mathop{\text{argmin}}_{ k=1\dots K } \rho(x_{i}, c_{k}) $$
  - Шаг б: Фиксируем $a(x_{i})$
$$c_{k} = \mathop{\text{argmin}}_{ c \in \mathbb{X} } \sum_{a(x)=k} \rho(x_{i}, c)  $$

При евклидовом расстоянии $c_{k} = \frac{1}{\sum_{i=1}^{\ell}} \sum_{i=1}^{\ell} [a(x_{i}) = k]x_{i}$.  Повторяем (а) и (б) до сходимости (сходимость гарантируется)

Плюсы:
- Быстрый
- Легко паралелится

Минусы:
- Зависит от инициализации
- Если признаки разного масштаба, то модет все поломаться
- Получаем кластеры слишком простой(эдлиптические) формы

#### 3. Иерархический подход

**1 шаг:** Инициализируем так, чтобы сначала у каждого объекта был свой собственный класстер
$$C_{(1)} = \{(x_{1}) \dots (x_{\ell})\}$$
...
**j-ый шаг:** Склеиваем похожие класстеры(как задавать похожесть выбираем сами)
$$C_{(j)} = \{(x_{1}) \dots (x_{\ell - j + 1})\}$$
$$(m, n) = \mathop{\text{argmin}}_{ 1 \leq m < n \leq \ell - j + 1 } d(X_{m}, X_{n})$$
$\implies$ Объединяем классы m, n
![alt](https://i.imgur.com/WbQXCoz.png)
