---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Случайный лес (Random Forest)

>[!info] Важно
>В Random Forest важно делать каждое следующее дерево сильно отличающимя от предыдущих! Также важно, чтобы bias каждого отдельно взятого дерева был минимальным (так как bootstap) минимизирует только variance

Важные моменты:
1. Деревья обучаются с большой глубиной (min_sample_leaf = 3)
2. Деревья обучаются на бутстрап подвыборкам
3. В каждой вершине в предикате признак $x_{j}$ будем выбирать не из всего признакового простанства, а из подвыборки размера k.

>[!info] Рекомендации по выбору k
>- Классификация: $k = \sqrt{ d }$
>- Регрессия: $k =  \frac{d}{3}$

- Для классификации: $\mathop{\text{argmax}}_{ y \in \mathbb{Y}} \sum_{n=1}^{N} b_{n}(x)$
- Для регрессии: $\frac{1}{N} \sum_{n=1}^{N} b_{n}(x)$

>[!info] RF - самый универсальный метод в ML. Почему?

### Гиперпараметры
- Число деревьев(N)
    ![](https://i.imgur.com/89zB5Pb.png)
    То есть даже число деревьев можно брвть просто большим

### Минусы клссичесткого алгоритма
- Очень долго обучается(!)
- Если $bias(b_{n})\gg 0$, то RF будет плох 
### Модификации RF

#### Out of bag
$b_{n}(x)$ обучается на некоторой подвыборке, то есть объекты, которые модель не видела на обучении $\implies$По ним можно померять ошибку
$$ OOB = \frac{1}{\ell} \sum_{i=1}^{\ell} L\left( y_{i}, \frac{1}{\sum_{n=1}^{N} [x_{n} \notin \mathbb{X}_{n}]} \sum_{n=1}^{N} [x_{i} \notin \mathbb{X}_{n}] b_{n} (x_{i}) \right) \sim LOO$$
#### Важность признаков (перестановочности)
*Примечание*: Можно использовать не только для RF.

![alt](https://i.imgur.com/StfBk67.png)

Важность $j$-того признака: $$Q_{test}^{j} - Q_{test} = q_{j}$$
-  $q_{j} \sim 0 \implies$ признак не важный
-  $q_{j} > 0 \implies$ признак важный
-  $q_{j} < 0 \implies$ kringe (или ошибка в коде)
