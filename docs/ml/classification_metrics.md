---
next:
  text: 'К содержанию'
  link: '/ml/ml_index'
prev: false
---

# Метрики качества классификации

### Доля верных ответов (accuracy) 

$$ \frac{1}{\ell} \sum [ a(x_{i}) = y_{i}]$$
У accuracy есть проблема с дисбалансом классов. Рассмотрим пример:
> +1: 50
> -1: 950

Рассмотрим оценку константной модели:
$$a(x)=+1 \implies accuracy =0.95$$
*Следствие*. Всегда вместе с accuracy нужно смотреть на баланс классов

#### Способы изменения улучшений

Когда мы улучшаем модель, есть 2 способа измерения улучшения:
> $r_{1}$ - доля ошибок до изменений
> $r_{2}$ - доля ошибок после изменений

Можно использовать $|r_{1} - r_{2}|$ или $\frac{r_{2} - r_{1}}{r_{1}}$ (Этим можно манипулировать хехе)
*Примечание*: Можно заметить, что в этом способе задания классов мы никак не учитываем ошибки разного рода.
### 2. Матрица ошибок (Confusion Matrix)

|           | y=+1 | y=-1 |
| --------- | ---- | ---- |
| a(x) = +1 | TP   | FP   |
| a(x) = -1 | FN   | TN   |

### 3. Accuracy/Precision/Recall

- *Accuracy*: сколько правильных предсказаний
$$\text{accuracy} = \frac{TP + TN}{TP + FP + TN + FN}$$
- *Presision*: насколько мы доверяем модели, когда она делает предсказание $a(x)=+1$ (точность)
$$\text{precision} = \frac{TP}{TP+FP}$$
- *Recall*: насколько модель покрывает положительный класс (полнота)
$$\text{recall} = \frac{TP}{TP + FN}$$

![alt](https://i.imgur.com/hPAgsoD.png)

### 4. F1-Score
#### Как балансировать между точностью и полнотой?

$$ a(x) = \text{sign}(b(x))$$

![alt](https://i.imgur.com/9OwNPwl.png)
> В этом случае t - гиперпараметр.

После обучения модели фиксируем веса и подбираем t
$$
\begin{equation}
    \left\{  
    \begin{array}{}  
        \text{presision}\to \max\\  
        \text{recall} \geq C\\  
    \end{array}  
    \right. 
\end{equation}
$$
$$
\begin{equation}
    \left\{  
    \begin{array}{}  
        \text{recall}\to \max\\  
        \text{presision} \geq C\\  
    \end{array}  
    \right. 
\end{equation}
$$
*Примечание*. Порог t не участвует в обучении!

#### Как объеденить точность и полноту?

1. Посчитать среднее арифмитическое:
    $$a=\frac{\text{precision}+\text{recall}}{2}$$
    Хорошая ли это модель? - Нет, так как при одном высоком параметре и другом низком модель будет выдавать такое же число, как при среднем между их результатами, что не позволяет нормально оценить получившуюся модель.
![alt](https://i.imgur.com/dVXWGIl.png)

2. Взять минимум:
    $$\mu = \min (\text{precision}, \text{recall})$$
    Минусы: Не различаем случаи, у которых при фиксированном параметре  другой параметр лучше (оценка такая же, как если бы брали оценку с параметром хуже)
3. Среднее гармоническое:
    $$F = 2 \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$$
    Обобщенный F1-score:
    $$F_{\alpha} = \frac{(1 + \alpha^{2})\text{precision} \cdot \text{recall}}{\alpha^{2} \text{precision} + \text{recall} }$$
    ![alt](https://i.imgur.com/ub8LzMN.png)
    Таким образом мы можем балансировать между различными классами
4. Среднее геометрическое:
    $$ G = \sqrt{ \text{precision} \cdot \text{recall} } $$
    $$G \leq F$$

### 5. (Дополнительно) Lift

$$\text{Lift} = \frac{\text{precision}}{\frac{TP + FP}{\ell}}$$
Функция $\text{Lift}$ используется для предсказаний оттока клиентов. В числителе стоит precision - он показывает, сколько звонков будет совершенно реально уходящим клиентам. В знаменателе стоить $\frac{TP + FP}{\ell}$ - этот пораметр показывает долю уходящих клиентов при случайном обзвоне. Таким образом метрика Lift опказывает, во сколько раз вырастет эффективность, если мы будем пользоваться моделью, а не звонить случайно.
